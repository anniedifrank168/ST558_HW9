[
  {
    "objectID": "HW9_AJD.html",
    "href": "HW9_AJD.html",
    "title": "HW9_AJD",
    "section": "",
    "text": "read_csv(\"SeoulBikeData.csv\",locale=locale(encoding=\"latin1\")) -&gt;bikedata\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n#1. Checking for missingness\nsum(is.na(bikedata))\n\n[1] 0\n\n  #no NAs \n\n#2. Checking the column types and values\n# head(bikedata)\n#all column types make sense, except the date-\nbikedata$Date &lt;- mdy(bikedata$Date)\n\nWarning: 5304 failed to parse.\n\nsummary(bikedata)\n\n      Date            Rented Bike Count      Hour       Temperature(°C) \n Min.   :2017-01-12   Min.   :   0.0    Min.   : 0.00   Min.   :-17.80  \n 1st Qu.:2018-03-02   1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50  \n Median :2018-06-05   Median : 504.5    Median :11.50   Median : 13.70  \n Mean   :2018-05-22   Mean   : 704.6    Mean   :11.50   Mean   : 12.88  \n 3rd Qu.:2018-09-08   3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50  \n Max.   :2018-12-11   Max.   :3556.0    Max.   :23.00   Max.   : 39.40  \n NA's   :5304                                                           \n  Humidity(%)    Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   : 0.00   Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:42.00   1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :57.00   Median :1.500    Median :1698     Median :  5.100          \n Mean   :58.23   Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:74.00   3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :98.00   Max.   :7.400    Max.   :2000     Max.   : 27.200          \n                                                                            \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n                                                                               \n   Holiday          Functioning Day   \n Length:8760        Length:8760       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n#numeric columns are fine, categoricals need converted to factors\nbikedata &lt;- bikedata %&gt;%\n  mutate(across(where(is.character), as.factor))\n\n# lapply(bikedata[sapply(bikedata, is.factor)], table)\n#categorical variables look fine\n\n#Renaming columns for ease\nnames(bikedata) &lt;-\n  c(\n    \"date\",\n    \"rental_count\",\n    \"hour\",\n    \"temperature\",\n    \"humidity\",\n    \"wind_speed\",\n    \"visability\",\n    \"dew_pt_temp\",\n    \"solar_radiation\",\n    \"rainfall\",\n    \"snowfall\",\n    \"seasons\",\n    \"holiday\",\n    \"functioning_day\"\n  )\n\n#additional summary statistics\n# table(bikedata$functioning_day)\n# table(bikedata$holiday)\n# table(bikedata$seasons)\n# \n# summary(bikedata$rental_count)\n# \n# bikedata %&gt;% group_by(functioning_day, holiday, seasons) %&gt;% summarize(count =\n#                                                                          n())\n# \n# bikedata %&gt;% group_by(functioning_day, rental_count) %&gt;% summarize()\n\n#filtering dataset on functioning days only\nbikedata &lt;- bikedata %&gt;% filter(functioning_day == \"Yes\")\n\n#summarize across the hours\nbikedata_summary &lt;-\n  bikedata %&gt;% group_by(date, seasons, holiday) %&gt;%\n  summarize(\n    #summing rental count, rainfall, and snowfall\n    total_rental_count = sum(rental_count, na.rm = TRUE),\n    total_rainfall = sum(rainfall, na.rm = TRUE),\n    total_snowfall = sum(snowfall, na.rm = TRUE),\n    \n    #calculate mean for other weather-related variables (temperature, dew_pt_temp, humidity, wind_speed, visability, solar_radiation)\n    avg_temperature = mean(temperature, na.rm = TRUE),\n    avg_humidity = mean(humidity, na.rm = TRUE),\n    avg_wind_speed = mean(wind_speed, na.rm = TRUE),\n    avg_dew_pt_temp = mean(dew_pt_temp, na.rm = TRUE),\n    avg_visability = mean(visability, na.rm = TRUE),\n    avg_solar_radiation = mean(solar_radiation, na.rm = TRUE)\n  ) %&gt;% ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n#Basic summary stats with new data \n# summary(bikedata_summary)\n# sum(is.na(bikedata_summary))\n #get rid of the na's \nbikedata_summary&lt;- bikedata_summary%&gt;% drop_na()\n\n  #correlation matrix between the numeric variables \nbike_numeric &lt;- bikedata_summary[sapply(bikedata_summary, is.numeric)]\n# cor(bike_numeric)\n\nThere are some obvious/expected correlations just due to this being a lot of weather data, such as a positive correlation between humidity and rainfall. Something I think is interesting is the positive correlation between dew pt. and total rental count (I hate a humid day) but again that’s probably just because, as we see, dew pt. has almost a completely positive correlation with temperature (0.97)\n\ncategorical_vars &lt;- c(\"seasons\",\"holiday\")\nnumeric_vars &lt;- names(bike_numeric)\n\n#Loop through each categorical variable to create a plot\n# for (cat_var in categorical_vars) {\n#   long_data&lt;- bikedata_summary %&gt;%\n#     select(all_of(c(cat_var,numeric_vars))) %&gt;% \n#     pivot_longer(cols = all_of(numeric_vars), names_to = \"numeric_variable\", values_to = \"value\")\n# \n#   #plot\n#   plot &lt;- ggplot(long_data, aes_string(x = cat_var, y = \"value\")) +\n#     geom_boxplot() +\n#     facet_wrap(~ numeric_variable, scales = \"free_y\") +\n#     labs(\n#       title = paste(\"Relationship Between\", cat_var, \"and Numeric Variables\"),\n#       x = cat_var,\n#       y = \"Value\"\n#     ) +\n#     theme_minimal()\n#   \n#   # Print the plot\n#   print(plot)\n# }\n\nThe relationship between snow and rainfall and whether it’s a holiday or not is weird! Other than that, there isn’t anything way out of the ordinary.\n\n#Looking at how total rent count relates to the other variables \n  \n#   #with numeric variables using GGally package \n# ggpairs(bike_numeric, title = \"Scatterplot Matrix: Total Rental Count and Numeric Variables\",\n#         #first time I printed everything was way too big for screen \n#         lower = list(continuous = wrap(\"points\", size = 0.5, alpha = 0.3)), #adjust point size for each scatter plot \n#         upper = list(continuous = wrap(\"cor\", size = 3)) #adjust size of the corr. statistics in each box \n# ) + theme(\n#    axis.text = element_text(size = 6), #smaller axis labels \n#    strip.text = element_text(size = 6) #smaller facet labels \n#  )\n# \n#   #with categorical variables \n# for (cat_var in categorical_vars) {\n#   #boxplot for each categorical variable\n#   plot &lt;- ggplot(bikedata_summary, aes_string(x = cat_var, y = \"total_rental_count\")) +\n#     geom_boxplot() +\n#     labs(\n#       title = paste(\"Total Rental Count by\", cat_var),\n#       x = cat_var,\n#       y = \"Total Rental Count\"\n#     ) +\n#     theme_minimal()\n#   \n#   #print the plot\n#   print(plot)\n# }\n\nThese both makes logistical sense.\n\n\n\n\n#split the data into training (75%) and testing (25%) sets, stratified by 'seasons'\nset.seed(123)  # Set a seed for reproducibility\nbike_split &lt;- initial_split(bikedata_summary, prop = 0.75, strata = seasons)\n\n#extract the training and testing sets\ntrain_data &lt;- training(bike_split)\ntest_data &lt;- testing(bike_split)\n\n#on the training data, create a 10-fold CV split \ncv_split &lt;- vfold_cv(train_data, v = 10, strata = seasons)\n\n#checking the structure of the cross-validation splits\ncv_split\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits          id    \n   &lt;list&gt;          &lt;chr&gt; \n 1 &lt;split [89/12]&gt; Fold01\n 2 &lt;split [89/12]&gt; Fold02\n 3 &lt;split [90/11]&gt; Fold03\n 4 &lt;split [90/11]&gt; Fold04\n 5 &lt;split [90/11]&gt; Fold05\n 6 &lt;split [91/10]&gt; Fold06\n 7 &lt;split [91/10]&gt; Fold07\n 8 &lt;split [93/8]&gt;  Fold08\n 9 &lt;split [93/8]&gt;  Fold09\n10 &lt;split [93/8]&gt;  Fold10\n\n\n\n\n\n\nHere, we will also fit the models using 10-fold cross-validation to determine the best model.\n\n\n#Recipe #1 ---------------\n\n  #fixing the date column\nbike_1_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  #extract the day of the week from the date variable\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  #create a new factor variable 'weekday_weekend'\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  #remove the intermediate 'dow' variable and the original 'date' variable\n  step_rm(date_dow, date) %&gt;% \n  #standardize numeric vars \n  step_normalize(all_numeric()) %&gt;% \n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n#prepare and bake the recipe\nfirst_recipe&lt;- prep(bike_1_recipe)\n# bike_1_recipe\nbake(first_recipe, bikedata_summary)\n\n# A tibble: 136 × 14\n   total_rainfall total_snowfall avg_temperature avg_humidity avg_wind_speed\n            &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n 1        -0.328          -0.199          -1.21        -0.885        -0.400 \n 2        -0.328          -0.199          -0.897        0.216        -0.143 \n 3         0.0249         -0.199          -0.607        1.56         -0.290 \n 4        -0.319          -0.199          -1.03        -0.431         2.42  \n 5        -0.328          -0.199          -1.37        -1.53         -1.03  \n 6        -0.213           2.11           -1.00         0.821        -1.64  \n 7        -0.328           2.59           -0.916        0.599        -0.173 \n 8        -0.328          -0.199          -1.32        -1.17          0.0597\n 9        -0.328          -0.199          -1.07        -0.877        -1.08  \n10         0.0337          8.51           -0.908        0.747         0.287 \n# ℹ 126 more rows\n# ℹ 9 more variables: avg_dew_pt_temp &lt;dbl&gt;, avg_visability &lt;dbl&gt;,\n#   avg_solar_radiation &lt;dbl&gt;, total_rental_count &lt;dbl&gt;, seasons_Spring &lt;dbl&gt;,\n#   seasons_Summer &lt;dbl&gt;, seasons_Winter &lt;dbl&gt;, holiday_No.Holiday &lt;dbl&gt;,\n#   weekday_weekend_Weekend &lt;dbl&gt;\n\n#Recipe #2 ---------------\n\nbike_2_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n  \n  ######add interaction terms \n  step_interact(~starts_with(\"seasons\"):holiday) %&gt;% \n  step_interact(~starts_with(\"seasons\"):avg_temperature) %&gt;% \n  step_interact(~avg_temperature:total_rainfall) %&gt;% \n\n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n# prep(bike_2_recipe)\n\n#Recipe #3 ---------------\n\nbike_3_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n\n  step_interact(~starts_with(\"seasons\"):holiday) %&gt;% \n  step_interact(~starts_with(\"seasons\"):avg_temperature) %&gt;% \n  step_interact(~avg_temperature:total_rainfall) %&gt;% \n  \n  ######add quadratic terms for each numeric predictor \n  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %&gt;% \n  \n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n# prep(bike_3_recipe)\n\n\n#Set up linear model fit to use the 'lm' engine \nrecipe_model&lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n#create recipe workflows   \nrecipe_1_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_1_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_1_wfl\n  \nrecipe_2_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_2_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_2_wfl\n  \nrecipe_3_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_3_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_3_wfl\n  \n\n#Fit the models using 10 fold CV via fit_resamples() \nrec_10_fold &lt;- vfold_cv(train_data, 10)\n  \nrec1_fits &lt;- recipe_1_wfl %&gt;% \n  fit_resamples(rec_10_fold)\n  \nrec2_fits &lt;- recipe_2_wfl %&gt;% \n  fit_resamples(rec_10_fold) \n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\nrec3_fits &lt;- recipe_3_wfl %&gt;% \n  fit_resamples(rec_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n#collect metrics of the three models \n# rbind(\n  # rec1_fits %&gt;% collect_metrics(),\n  # rec2_fits %&gt;% collect_metrics(),\n  # rec3_fits %&gt;% collect_metrics())\n\nLooking at the metrics of the three models, the best model is model 2 with the lowest rmse and highest value of R-squared.\n\n\n\n\nHere, we will fit the best model to the entire training data set\n\nwe will additionally compute the RMSE metric on the test set and obtain the model (fit on the entire training set) coefficient table\n\n\n\n#fitting on the training set bbnfgb\nfinal_fit &lt;- recipe_2_wfl %&gt;% last_fit(split = bike_split, metrics = metric_set(rmse,mae))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n#finding test set metrics\nfinal_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.301 Preprocessor1_Model1\n2 mae     standard       0.224 Preprocessor1_Model1\n\n#storing metrics as a table \nMLR_coef &lt;- final_fit %&gt;% collect_metrics()\n\n#obtaining the final model fit \nfinal_MLR_model &lt;- final_fit %&gt;% extract_fit_parsnip()\n\n#tidy table of coefficients \ntidy(final_MLR_model)\n\n# A tibble: 24 × 5\n   term                                estimate std.error statistic    p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 (Intercept)                          0.731      0.347      2.11  0.0380    \n 2 total_rainfall                      -0.325      0.0820    -3.96  0.000160  \n 3 total_snowfall                      -0.00857    0.0367    -0.233 0.816     \n 4 avg_temperature                     -1.57       0.966     -1.62  0.109     \n 5 avg_humidity                        -0.586      0.336     -1.75  0.0847    \n 6 avg_wind_speed                      -0.0361     0.0413    -0.875 0.384     \n 7 avg_dew_pt_temp                      2.18       1.10       1.99  0.0502    \n 8 avg_visability                       0.0786     0.0550     1.43  0.157     \n 9 avg_solar_radiation                  0.373      0.0714     5.23  0.00000131\n10 `seasonsSpring_x_holidayNo Holiday` -0.404      0.116     -3.47  0.000829  \n# ℹ 14 more rows\n\n\n\n\n\nThe RMSE metric of the test set is 0.301. The R-squared value is 0.91, meaning the model explains 91% of the variance in the total_rental_count. The coefficient table shows each coefficient/estimate for the predictors in the model."
  },
  {
    "objectID": "HW9_AJD.html#homework-9-extension-starts-around-line-334",
    "href": "HW9_AJD.html#homework-9-extension-starts-around-line-334",
    "title": "HW9_AJD",
    "section": "",
    "text": "read_csv(\"SeoulBikeData.csv\",locale=locale(encoding=\"latin1\")) -&gt;bikedata\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n#1. Checking for missingness\nsum(is.na(bikedata))\n\n[1] 0\n\n  #no NAs \n\n#2. Checking the column types and values\n# head(bikedata)\n#all column types make sense, except the date-\nbikedata$Date &lt;- mdy(bikedata$Date)\n\nWarning: 5304 failed to parse.\n\nsummary(bikedata)\n\n      Date            Rented Bike Count      Hour       Temperature(°C) \n Min.   :2017-01-12   Min.   :   0.0    Min.   : 0.00   Min.   :-17.80  \n 1st Qu.:2018-03-02   1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50  \n Median :2018-06-05   Median : 504.5    Median :11.50   Median : 13.70  \n Mean   :2018-05-22   Mean   : 704.6    Mean   :11.50   Mean   : 12.88  \n 3rd Qu.:2018-09-08   3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50  \n Max.   :2018-12-11   Max.   :3556.0    Max.   :23.00   Max.   : 39.40  \n NA's   :5304                                                           \n  Humidity(%)    Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   : 0.00   Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:42.00   1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :57.00   Median :1.500    Median :1698     Median :  5.100          \n Mean   :58.23   Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:74.00   3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :98.00   Max.   :7.400    Max.   :2000     Max.   : 27.200          \n                                                                            \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n                                                                               \n   Holiday          Functioning Day   \n Length:8760        Length:8760       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n#numeric columns are fine, categoricals need converted to factors\nbikedata &lt;- bikedata %&gt;%\n  mutate(across(where(is.character), as.factor))\n\n# lapply(bikedata[sapply(bikedata, is.factor)], table)\n#categorical variables look fine\n\n#Renaming columns for ease\nnames(bikedata) &lt;-\n  c(\n    \"date\",\n    \"rental_count\",\n    \"hour\",\n    \"temperature\",\n    \"humidity\",\n    \"wind_speed\",\n    \"visability\",\n    \"dew_pt_temp\",\n    \"solar_radiation\",\n    \"rainfall\",\n    \"snowfall\",\n    \"seasons\",\n    \"holiday\",\n    \"functioning_day\"\n  )\n\n#additional summary statistics\n# table(bikedata$functioning_day)\n# table(bikedata$holiday)\n# table(bikedata$seasons)\n# \n# summary(bikedata$rental_count)\n# \n# bikedata %&gt;% group_by(functioning_day, holiday, seasons) %&gt;% summarize(count =\n#                                                                          n())\n# \n# bikedata %&gt;% group_by(functioning_day, rental_count) %&gt;% summarize()\n\n#filtering dataset on functioning days only\nbikedata &lt;- bikedata %&gt;% filter(functioning_day == \"Yes\")\n\n#summarize across the hours\nbikedata_summary &lt;-\n  bikedata %&gt;% group_by(date, seasons, holiday) %&gt;%\n  summarize(\n    #summing rental count, rainfall, and snowfall\n    total_rental_count = sum(rental_count, na.rm = TRUE),\n    total_rainfall = sum(rainfall, na.rm = TRUE),\n    total_snowfall = sum(snowfall, na.rm = TRUE),\n    \n    #calculate mean for other weather-related variables (temperature, dew_pt_temp, humidity, wind_speed, visability, solar_radiation)\n    avg_temperature = mean(temperature, na.rm = TRUE),\n    avg_humidity = mean(humidity, na.rm = TRUE),\n    avg_wind_speed = mean(wind_speed, na.rm = TRUE),\n    avg_dew_pt_temp = mean(dew_pt_temp, na.rm = TRUE),\n    avg_visability = mean(visability, na.rm = TRUE),\n    avg_solar_radiation = mean(solar_radiation, na.rm = TRUE)\n  ) %&gt;% ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n#Basic summary stats with new data \n# summary(bikedata_summary)\n# sum(is.na(bikedata_summary))\n #get rid of the na's \nbikedata_summary&lt;- bikedata_summary%&gt;% drop_na()\n\n  #correlation matrix between the numeric variables \nbike_numeric &lt;- bikedata_summary[sapply(bikedata_summary, is.numeric)]\n# cor(bike_numeric)\n\nThere are some obvious/expected correlations just due to this being a lot of weather data, such as a positive correlation between humidity and rainfall. Something I think is interesting is the positive correlation between dew pt. and total rental count (I hate a humid day) but again that’s probably just because, as we see, dew pt. has almost a completely positive correlation with temperature (0.97)\n\ncategorical_vars &lt;- c(\"seasons\",\"holiday\")\nnumeric_vars &lt;- names(bike_numeric)\n\n#Loop through each categorical variable to create a plot\n# for (cat_var in categorical_vars) {\n#   long_data&lt;- bikedata_summary %&gt;%\n#     select(all_of(c(cat_var,numeric_vars))) %&gt;% \n#     pivot_longer(cols = all_of(numeric_vars), names_to = \"numeric_variable\", values_to = \"value\")\n# \n#   #plot\n#   plot &lt;- ggplot(long_data, aes_string(x = cat_var, y = \"value\")) +\n#     geom_boxplot() +\n#     facet_wrap(~ numeric_variable, scales = \"free_y\") +\n#     labs(\n#       title = paste(\"Relationship Between\", cat_var, \"and Numeric Variables\"),\n#       x = cat_var,\n#       y = \"Value\"\n#     ) +\n#     theme_minimal()\n#   \n#   # Print the plot\n#   print(plot)\n# }\n\nThe relationship between snow and rainfall and whether it’s a holiday or not is weird! Other than that, there isn’t anything way out of the ordinary.\n\n#Looking at how total rent count relates to the other variables \n  \n#   #with numeric variables using GGally package \n# ggpairs(bike_numeric, title = \"Scatterplot Matrix: Total Rental Count and Numeric Variables\",\n#         #first time I printed everything was way too big for screen \n#         lower = list(continuous = wrap(\"points\", size = 0.5, alpha = 0.3)), #adjust point size for each scatter plot \n#         upper = list(continuous = wrap(\"cor\", size = 3)) #adjust size of the corr. statistics in each box \n# ) + theme(\n#    axis.text = element_text(size = 6), #smaller axis labels \n#    strip.text = element_text(size = 6) #smaller facet labels \n#  )\n# \n#   #with categorical variables \n# for (cat_var in categorical_vars) {\n#   #boxplot for each categorical variable\n#   plot &lt;- ggplot(bikedata_summary, aes_string(x = cat_var, y = \"total_rental_count\")) +\n#     geom_boxplot() +\n#     labs(\n#       title = paste(\"Total Rental Count by\", cat_var),\n#       x = cat_var,\n#       y = \"Total Rental Count\"\n#     ) +\n#     theme_minimal()\n#   \n#   #print the plot\n#   print(plot)\n# }\n\nThese both makes logistical sense.\n\n\n\n\n#split the data into training (75%) and testing (25%) sets, stratified by 'seasons'\nset.seed(123)  # Set a seed for reproducibility\nbike_split &lt;- initial_split(bikedata_summary, prop = 0.75, strata = seasons)\n\n#extract the training and testing sets\ntrain_data &lt;- training(bike_split)\ntest_data &lt;- testing(bike_split)\n\n#on the training data, create a 10-fold CV split \ncv_split &lt;- vfold_cv(train_data, v = 10, strata = seasons)\n\n#checking the structure of the cross-validation splits\ncv_split\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits          id    \n   &lt;list&gt;          &lt;chr&gt; \n 1 &lt;split [89/12]&gt; Fold01\n 2 &lt;split [89/12]&gt; Fold02\n 3 &lt;split [90/11]&gt; Fold03\n 4 &lt;split [90/11]&gt; Fold04\n 5 &lt;split [90/11]&gt; Fold05\n 6 &lt;split [91/10]&gt; Fold06\n 7 &lt;split [91/10]&gt; Fold07\n 8 &lt;split [93/8]&gt;  Fold08\n 9 &lt;split [93/8]&gt;  Fold09\n10 &lt;split [93/8]&gt;  Fold10\n\n\n\n\n\n\nHere, we will also fit the models using 10-fold cross-validation to determine the best model.\n\n\n#Recipe #1 ---------------\n\n  #fixing the date column\nbike_1_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  #extract the day of the week from the date variable\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  #create a new factor variable 'weekday_weekend'\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  #remove the intermediate 'dow' variable and the original 'date' variable\n  step_rm(date_dow, date) %&gt;% \n  #standardize numeric vars \n  step_normalize(all_numeric()) %&gt;% \n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n#prepare and bake the recipe\nfirst_recipe&lt;- prep(bike_1_recipe)\n# bike_1_recipe\nbake(first_recipe, bikedata_summary)\n\n# A tibble: 136 × 14\n   total_rainfall total_snowfall avg_temperature avg_humidity avg_wind_speed\n            &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n 1        -0.328          -0.199          -1.21        -0.885        -0.400 \n 2        -0.328          -0.199          -0.897        0.216        -0.143 \n 3         0.0249         -0.199          -0.607        1.56         -0.290 \n 4        -0.319          -0.199          -1.03        -0.431         2.42  \n 5        -0.328          -0.199          -1.37        -1.53         -1.03  \n 6        -0.213           2.11           -1.00         0.821        -1.64  \n 7        -0.328           2.59           -0.916        0.599        -0.173 \n 8        -0.328          -0.199          -1.32        -1.17          0.0597\n 9        -0.328          -0.199          -1.07        -0.877        -1.08  \n10         0.0337          8.51           -0.908        0.747         0.287 \n# ℹ 126 more rows\n# ℹ 9 more variables: avg_dew_pt_temp &lt;dbl&gt;, avg_visability &lt;dbl&gt;,\n#   avg_solar_radiation &lt;dbl&gt;, total_rental_count &lt;dbl&gt;, seasons_Spring &lt;dbl&gt;,\n#   seasons_Summer &lt;dbl&gt;, seasons_Winter &lt;dbl&gt;, holiday_No.Holiday &lt;dbl&gt;,\n#   weekday_weekend_Weekend &lt;dbl&gt;\n\n#Recipe #2 ---------------\n\nbike_2_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n  \n  ######add interaction terms \n  step_interact(~starts_with(\"seasons\"):holiday) %&gt;% \n  step_interact(~starts_with(\"seasons\"):avg_temperature) %&gt;% \n  step_interact(~avg_temperature:total_rainfall) %&gt;% \n\n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n# prep(bike_2_recipe)\n\n#Recipe #3 ---------------\n\nbike_3_recipe &lt;- recipe(total_rental_count ~ ., data = bikedata_summary) %&gt;%\n  step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n\n  step_interact(~starts_with(\"seasons\"):holiday) %&gt;% \n  step_interact(~starts_with(\"seasons\"):avg_temperature) %&gt;% \n  step_interact(~avg_temperature:total_rainfall) %&gt;% \n  \n  ######add quadratic terms for each numeric predictor \n  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %&gt;% \n  \n  #dummy variables \n  step_dummy(all_nominal_predictors())\n\n# prep(bike_3_recipe)\n\n\n#Set up linear model fit to use the 'lm' engine \nrecipe_model&lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\n#create recipe workflows   \nrecipe_1_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_1_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_1_wfl\n  \nrecipe_2_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_2_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_2_wfl\n  \nrecipe_3_wfl &lt;- workflow() %&gt;% \n  add_recipe(bike_3_recipe) %&gt;% \n  add_model(recipe_model)\n# recipe_3_wfl\n  \n\n#Fit the models using 10 fold CV via fit_resamples() \nrec_10_fold &lt;- vfold_cv(train_data, 10)\n  \nrec1_fits &lt;- recipe_1_wfl %&gt;% \n  fit_resamples(rec_10_fold)\n  \nrec2_fits &lt;- recipe_2_wfl %&gt;% \n  fit_resamples(rec_10_fold) \n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\nrec3_fits &lt;- recipe_3_wfl %&gt;% \n  fit_resamples(rec_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n#collect metrics of the three models \n# rbind(\n  # rec1_fits %&gt;% collect_metrics(),\n  # rec2_fits %&gt;% collect_metrics(),\n  # rec3_fits %&gt;% collect_metrics())\n\nLooking at the metrics of the three models, the best model is model 2 with the lowest rmse and highest value of R-squared.\n\n\n\n\nHere, we will fit the best model to the entire training data set\n\nwe will additionally compute the RMSE metric on the test set and obtain the model (fit on the entire training set) coefficient table\n\n\n\n#fitting on the training set bbnfgb\nfinal_fit &lt;- recipe_2_wfl %&gt;% last_fit(split = bike_split, metrics = metric_set(rmse,mae))\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n#finding test set metrics\nfinal_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.301 Preprocessor1_Model1\n2 mae     standard       0.224 Preprocessor1_Model1\n\n#storing metrics as a table \nMLR_coef &lt;- final_fit %&gt;% collect_metrics()\n\n#obtaining the final model fit \nfinal_MLR_model &lt;- final_fit %&gt;% extract_fit_parsnip()\n\n#tidy table of coefficients \ntidy(final_MLR_model)\n\n# A tibble: 24 × 5\n   term                                estimate std.error statistic    p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 (Intercept)                          0.731      0.347      2.11  0.0380    \n 2 total_rainfall                      -0.325      0.0820    -3.96  0.000160  \n 3 total_snowfall                      -0.00857    0.0367    -0.233 0.816     \n 4 avg_temperature                     -1.57       0.966     -1.62  0.109     \n 5 avg_humidity                        -0.586      0.336     -1.75  0.0847    \n 6 avg_wind_speed                      -0.0361     0.0413    -0.875 0.384     \n 7 avg_dew_pt_temp                      2.18       1.10       1.99  0.0502    \n 8 avg_visability                       0.0786     0.0550     1.43  0.157     \n 9 avg_solar_radiation                  0.373      0.0714     5.23  0.00000131\n10 `seasonsSpring_x_holidayNo Holiday` -0.404      0.116     -3.47  0.000829  \n# ℹ 14 more rows\n\n\n\n\n\nThe RMSE metric of the test set is 0.301. The R-squared value is 0.91, meaning the model explains 91% of the variance in the total_rental_count. The coefficient table shows each coefficient/estimate for the predictors in the model."
  },
  {
    "objectID": "HW9_AJD.html#homework-9-extension",
    "href": "HW9_AJD.html#homework-9-extension",
    "title": "HW9_AJD",
    "section": "Homework 9 Extension",
    "text": "Homework 9 Extension\n\n6. Tuned LASSO Model\n\n#set up how we'll fit LASSO model \nlasso_recipe &lt;- recipe(total_rental_count ~., data = bikedata_summary) %&gt;% \n    step_date(date, features = \"dow\", label = TRUE) %&gt;%\n  step_mutate(\n    weekday_weekend = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;% \n  step_normalize(all_numeric()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n#create a model instance \nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_engine(\"glmnet\")\n\n#create workflow\nlasso_wkf &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%\n  add_model(lasso_spec)\nlasso_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n#fitting the model using tune_grid (fitting model to CV folds)\nlasso_grid &lt;- lasso_wkf %&gt;%\n  tune_grid(resamples= cv_split,\n            grid=grid_regular(penalty(), levels = 200))\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n# lasso_grid\n\n#collect metrics \nlasso_grid %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model009\n10 2.83e-10 rmse    standard   0.467    10  0.0370 Preprocessor1_Model010\n# ℹ 190 more rows\n\n#pulling out the best lasso model\nlowest_rmse &lt;- lasso_grid %&gt;%\n  select_best(metric = \"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n  penalty .config               \n    &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00691 Preprocessor1_Model157\n\n#fit the best model on the entire training set \nlasso_final &lt;- lasso_wkf %&gt;%\n  finalize_workflow(lowest_rmse) %&gt;%\n  fit(train_data)\n# tidy(lasso_final)\n\nFinal LASSO model coefficient table:\n\nlasso_wkf %&gt;% finalize_workflow(lowest_rmse) %&gt;% last_fit(bike_split, metrics = metric_set(rmse,mae)) %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.441 Preprocessor1_Model1\n2 mae     standard       0.370 Preprocessor1_Model1\n\n#storing as a table- \nlasso_coef &lt;- lasso_wkf %&gt;% finalize_workflow(lowest_rmse) %&gt;% last_fit(bike_split, metrics = metric_set(rmse,mae)) %&gt;% collect_metrics()\n\n\n\n7. Tuned Regression Tree Model\n\n#same recipe as used in lasso \n\n#define model and engine\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n  \n#create workflow\nregtree_wkf &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%\n  add_model(tree_mod)\n\n#use CV to select tuning parameters\ntemp &lt;- regtree_wkf %&gt;% tune_grid(resamples = cv_split)\ntemp %&gt;% collect_metrics()\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        1.76e-10         11 rmse    standard   0.460    10  0.0450 Preprocess…\n 2        1.76e-10         11 rsq     standard   0.767    10  0.0460 Preprocess…\n 3        2.02e- 9          8 rmse    standard   0.460    10  0.0450 Preprocess…\n 4        2.02e- 9          8 rsq     standard   0.767    10  0.0460 Preprocess…\n 5        1.24e- 7          2 rmse    standard   0.541    10  0.0533 Preprocess…\n 6        1.24e- 7          2 rsq     standard   0.708    10  0.0617 Preprocess…\n 7        9.03e- 4          6 rmse    standard   0.460    10  0.0450 Preprocess…\n 8        9.03e- 4          6 rsq     standard   0.767    10  0.0460 Preprocess…\n 9        5.69e- 6          2 rmse    standard   0.541    10  0.0533 Preprocess…\n10        5.69e- 6          2 rsq     standard   0.708    10  0.0617 Preprocess…\n11        4.12e- 2          4 rmse    standard   0.507    10  0.0464 Preprocess…\n12        4.12e- 2          4 rsq     standard   0.724    10  0.0539 Preprocess…\n13        6.03e- 7         12 rmse    standard   0.460    10  0.0450 Preprocess…\n14        6.03e- 7         12 rsq     standard   0.767    10  0.0460 Preprocess…\n15        7.87e- 5         13 rmse    standard   0.460    10  0.0450 Preprocess…\n16        7.87e- 5         13 rsq     standard   0.767    10  0.0460 Preprocess…\n17        5.80e- 3          7 rmse    standard   0.459    10  0.0456 Preprocess…\n18        5.80e- 3          7 rsq     standard   0.768    10  0.0468 Preprocess…\n19        7.84e- 9         14 rmse    standard   0.460    10  0.0450 Preprocess…\n20        7.84e- 9         14 rsq     standard   0.767    10  0.0460 Preprocess…\n\nregtree_grid &lt;- grid_regular(cost_complexity(),\n                             tree_depth(),\n                             levels = c(10, 5))\n\nregtree_fits &lt;- regtree_wkf %&gt;%\n  tune_grid(resamples = cv_split,\n            grid = regtree_grid)\n\n# regtree_fits %&gt;% collect_metrics() %&gt;% filter(.metric == \"rmse\") %&gt;% arrange(mean)\n\n#grab the best model's tuning parameters\nregtree_best_param &lt;- select_best(regtree_fits)\n\nWarning in select_best(regtree_fits): No value of `metric` was given; \"rmse\"\nwill be used.\n\nregtree_best_param\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          8 Preprocessor1_Model21\n\n#finalize best model on the training set \n  #fit the best model\nregtree_final_wkf &lt;- regtree_wkf %&gt;% \n  finalize_workflow(regtree_best_param)\n  #fit on entire training set \nregtree_final_fit &lt;- regtree_final_wkf %&gt;% \n  last_fit(bike_split, metrics = metric_set(rmse, mae))\nregtree_final_fit %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.340 Preprocessor1_Model1\n2 mae     standard       0.241 Preprocessor1_Model1\n\n#storing as a table- \nregtree_coef &lt;- regtree_final_fit %&gt;% collect_metrics()\n\nPlot of the final regression tree model:\n\nregtree_final_fit %&gt;% \n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint=FALSE)\n\n\n\n\n\n\n8. Tuned Bagged Tree Model\n\n#same recipe used as previous \n\n#define model type/engine\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n= 10, cost_complexity = tune()) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\n#create workflow\nbag_wkf &lt;- workflow() %&gt;% \n  add_recipe(lasso_recipe) %&gt;%\n  add_model(bag_spec)\n\n#fit to cv folds \nbag_fit &lt;- bag_wkf %&gt;% \n  tune_grid(resamples = cv_split, \n            grid = grid_regular(cost_complexity(),\n                                levels = 15),\n            metrics = metric_set(rmse, mae))\n\n# bag_fit %&gt;% \n  # collect_metrics() %&gt;% \n  # filter(.metric == \"rmse\") %&gt;% \n  # arrange(mean)\n\n#retrieve best tuning parameter\nbag_best_param &lt;- select_best(bag_fit)\n\nWarning in select_best(bag_fit): No value of `metric` was given; \"rmse\" will be\nused.\n\n#refit on entire training set \nbag_final_wkf &lt;- bag_wkf %&gt;% \n  finalize_workflow(bag_best_param)\n\nbag_final_fit &lt;- bag_final_wkf %&gt;% \n  last_fit(bike_split, metrics = metric_set(rmse,mae))\n\ncollect_metrics(bag_final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.301 Preprocessor1_Model1\n2 mae     standard       0.247 Preprocessor1_Model1\n\n#storing as table \nbag_coef&lt;- collect_metrics(bag_final_fit)\n\nVariable Importance Plot:\n\n#extract the final model and plot \nbag_final_model &lt;- extract_fit_engine(bag_final_fit)\nbag_final_model$imp %&gt;%\n  mutate(term=factor(term,levels=term)) %&gt;% \n  ggplot(aes(x=term,y=value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\n\n\n#avg temp has biggest play in the outcome! \n\n\n\n9. Tuned Random Forest Model\n\n#same recipe as previous \n#model specs - i found i had to add in the extra importance object to be able to get the variable importance info later on \nrf_spec &lt;- rand_forest(mtry=tune()) %&gt;% \n  set_engine(\"ranger\", importance = \"permutation\") %&gt;% \n  set_mode(\"regression\")\n\n#create workflow \nrf_wkf &lt;- workflow() %&gt;% \n  add_recipe(lasso_recipe) %&gt;% \n  add_model(rf_spec)\n\n#fit to cv folds\nrf_fit &lt;- rf_wkf %&gt;% \n  tune_grid(resamples = cv_split, \n            grid = 7,\n            metrics = metric_set(rmse,mae))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n#looking at metrics across folds \nrf_fit %&gt;% collect_metrics() %&gt;% arrange(mean)\n\n# A tibble: 14 × 7\n    mtry .metric .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1    10 mae     standard   0.257    10  0.0205 Preprocessor1_Model2\n 2    12 mae     standard   0.259    10  0.0191 Preprocessor1_Model6\n 3     9 mae     standard   0.261    10  0.0203 Preprocessor1_Model5\n 4     7 mae     standard   0.264    10  0.0211 Preprocessor1_Model7\n 5     5 mae     standard   0.269    10  0.0225 Preprocessor1_Model3\n 6     4 mae     standard   0.284    10  0.0229 Preprocessor1_Model4\n 7    10 rmse    standard   0.324    10  0.0242 Preprocessor1_Model2\n 8     2 mae     standard   0.325    10  0.0248 Preprocessor1_Model1\n 9    12 rmse    standard   0.326    10  0.0226 Preprocessor1_Model6\n10     9 rmse    standard   0.327    10  0.0238 Preprocessor1_Model5\n11     7 rmse    standard   0.328    10  0.0245 Preprocessor1_Model7\n12     5 rmse    standard   0.333    10  0.0266 Preprocessor1_Model3\n13     4 rmse    standard   0.350    10  0.0282 Preprocessor1_Model4\n14     2 rmse    standard   0.400    10  0.0299 Preprocessor1_Model1\n\n#get best tuning parameter\nrf_best_param &lt;- select_best(rf_fit)\n\nWarning in select_best(rf_fit): No value of `metric` was given; \"rmse\" will be\nused.\n\n#refit on entire training set with this param\nrf_final_wkf &lt;- rf_wkf %&gt;% \n  finalize_workflow(rf_best_param)\n\nrf_final_fit &lt;- rf_final_wkf %&gt;% \n  last_fit(bike_split, metrics = metric_set(rmse,mae))\ncollect_metrics(rf_final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.255 Preprocessor1_Model1\n2 mae     standard       0.204 Preprocessor1_Model1\n\n#saving as table \nrf_coef&lt;- collect_metrics(rf_final_fit)\n\nVariable Importance Plot:\n\n#extract the final model and plot \nrf_final_model &lt;- extract_fit_engine(rf_final_fit)\n#earlier used code didnt work for this model\nimpor_data&lt;- as.data.frame(rf_final_model$variable.importance) %&gt;% rownames_to_column(\"Variable\") %&gt;%\n  rename(value = \"rf_final_model$variable.importance\") %&gt;% \n  arrange(desc(value)) \n\nggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() \n\n\n\n#This model also suggests temp has the biggest roll, by a lot \n\n\n\n10. Comparing all final models (including the best MLR model from homework 8)\n\n#add model names as a new column\nMLR_coef &lt;- MLR_coef %&gt;% mutate(model = \"MLR\")\nlasso_coef &lt;- lasso_coef %&gt;% mutate(model = \"Lasso\")\nregtree_coef &lt;- regtree_coef %&gt;% mutate(model = \"Regression Tree\")\nbag_coef &lt;- bag_coef %&gt;% mutate(model = \"Bagging\")\nrf_coef &lt;- rf_coef %&gt;% mutate(model = \"Random Forest\")\nrbind(MLR_coef,lasso_coef,regtree_coef,bag_coef,rf_coef)\n\n# A tibble: 10 × 5\n   .metric .estimator .estimate .config              model          \n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;          \n 1 rmse    standard       0.301 Preprocessor1_Model1 MLR            \n 2 mae     standard       0.224 Preprocessor1_Model1 MLR            \n 3 rmse    standard       0.441 Preprocessor1_Model1 Lasso          \n 4 mae     standard       0.370 Preprocessor1_Model1 Lasso          \n 5 rmse    standard       0.340 Preprocessor1_Model1 Regression Tree\n 6 mae     standard       0.241 Preprocessor1_Model1 Regression Tree\n 7 rmse    standard       0.301 Preprocessor1_Model1 Bagging        \n 8 mae     standard       0.247 Preprocessor1_Model1 Bagging        \n 9 rmse    standard       0.255 Preprocessor1_Model1 Random Forest  \n10 mae     standard       0.204 Preprocessor1_Model1 Random Forest  \n\n\nThe random forest model did the best, with a RMSE of 0.250 and a MAE of 0.206. The bagging tree method was very close to this method on these metrics. The Lasso model performed the worst."
  },
  {
    "objectID": "HW9_AJD.html#fitting-the-overall-best-model-to-the-entire-data-set",
    "href": "HW9_AJD.html#fitting-the-overall-best-model-to-the-entire-data-set",
    "title": "HW9_AJD",
    "section": "11. Fitting the overall best model to the entire data set",
    "text": "11. Fitting the overall best model to the entire data set\n\nbest_fit_ever &lt;- bag_final_wkf %&gt;% \n  fit(bikedata_summary)\nbest_fit_ever\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: bag_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBagged CART (regression with 11 members)\n\nVariable importance scores include:\n\n# A tibble: 13 × 4\n   term                      value std.error  used\n   &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 avg_temperature         92.2      3.08       11\n 2 avg_dew_pt_temp         81.4      2.79       11\n 3 avg_humidity            60.0      2.99       11\n 4 seasons_Winter          57.6      3.25       11\n 5 avg_solar_radiation     57.3      2.72       11\n 6 avg_wind_speed          23.5      2.94       11\n 7 total_rainfall          21.9      2.81       11\n 8 avg_visability          10.2      1.32       11\n 9 seasons_Summer           8.98     4.77        9\n10 total_snowfall           3.63     2.31        6\n11 seasons_Spring           2.67     0.505      11\n12 weekday_weekend_Weekend  0.279    0.0851      9\n13 holiday_No.Holiday       0.0123   0.00642     2\n\nbest_final_model &lt;- extract_fit_engine(best_fit_ever)\nbest_final_model$imp %&gt;%\n  mutate(term=factor(term,levels=term)) %&gt;% \n  ggplot(aes(x=term,y=value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip()\n\n\n\n#woohoo"
  }
]