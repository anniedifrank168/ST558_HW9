---
title: "HW9_AJD"
format: html
editor: visual
---

```{r setup, include=FALSE}

#pacman to check whether packages are installed, if not load them
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(dplyr,
 tidyverse,
 ggplot2,
 readr,
 psych,
 lubridate,
 GGally,
 tidymodels,
 recipes,
 tree,
 rpart,
 rpart.plot,
 baguette,
 ranger)

```

## Homework 9 extension starts around line 334!

### \*Note, coefficient table for the best MLR model is just above the homework 9 extension!

### 1. Reading in the data

```{r}
read_csv("SeoulBikeData.csv",locale=locale(encoding="latin1")) ->bikedata
```

### 2. EDA - for Homework 9, commenting out most of the previously done summary stats

```{r}
#1. Checking for missingness
sum(is.na(bikedata))
  #no NAs 

#2. Checking the column types and values
# head(bikedata)
#all column types make sense, except the date-
bikedata$Date <- mdy(bikedata$Date)

summary(bikedata)
#numeric columns are fine, categoricals need converted to factors
bikedata <- bikedata %>%
  mutate(across(where(is.character), as.factor))

# lapply(bikedata[sapply(bikedata, is.factor)], table)
#categorical variables look fine

#Renaming columns for ease
names(bikedata) <-
  c(
    "date",
    "rental_count",
    "hour",
    "temperature",
    "humidity",
    "wind_speed",
    "visability",
    "dew_pt_temp",
    "solar_radiation",
    "rainfall",
    "snowfall",
    "seasons",
    "holiday",
    "functioning_day"
  )

#additional summary statistics
# table(bikedata$functioning_day)
# table(bikedata$holiday)
# table(bikedata$seasons)
# 
# summary(bikedata$rental_count)
# 
# bikedata %>% group_by(functioning_day, holiday, seasons) %>% summarize(count =
#                                                                          n())
# 
# bikedata %>% group_by(functioning_day, rental_count) %>% summarize()

#filtering dataset on functioning days only
bikedata <- bikedata %>% filter(functioning_day == "Yes")

#summarize across the hours
bikedata_summary <-
  bikedata %>% group_by(date, seasons, holiday) %>%
  summarize(
    #summing rental count, rainfall, and snowfall
    total_rental_count = sum(rental_count, na.rm = TRUE),
    total_rainfall = sum(rainfall, na.rm = TRUE),
    total_snowfall = sum(snowfall, na.rm = TRUE),
    
    #calculate mean for other weather-related variables (temperature, dew_pt_temp, humidity, wind_speed, visability, solar_radiation)
    avg_temperature = mean(temperature, na.rm = TRUE),
    avg_humidity = mean(humidity, na.rm = TRUE),
    avg_wind_speed = mean(wind_speed, na.rm = TRUE),
    avg_dew_pt_temp = mean(dew_pt_temp, na.rm = TRUE),
    avg_visability = mean(visability, na.rm = TRUE),
    avg_solar_radiation = mean(solar_radiation, na.rm = TRUE)
  ) %>% ungroup()

#Basic summary stats with new data 
# summary(bikedata_summary)
# sum(is.na(bikedata_summary))
 #get rid of the na's 
bikedata_summary<- bikedata_summary%>% drop_na()

  #correlation matrix between the numeric variables 
bike_numeric <- bikedata_summary[sapply(bikedata_summary, is.numeric)]
# cor(bike_numeric)

```

There are some obvious/expected correlations just due to this being a lot of weather data, such as a positive correlation between humidity and rainfall. Something I think is interesting is the positive correlation between dew pt. and total rental count (I hate a humid day) but again that's probably just because, as we see, dew pt. has almost a completely positive correlation with temperature (0.97)

```{r}
categorical_vars <- c("seasons","holiday")
numeric_vars <- names(bike_numeric)

#Loop through each categorical variable to create a plot
# for (cat_var in categorical_vars) {
#   long_data<- bikedata_summary %>%
#     select(all_of(c(cat_var,numeric_vars))) %>% 
#     pivot_longer(cols = all_of(numeric_vars), names_to = "numeric_variable", values_to = "value")
# 
#   #plot
#   plot <- ggplot(long_data, aes_string(x = cat_var, y = "value")) +
#     geom_boxplot() +
#     facet_wrap(~ numeric_variable, scales = "free_y") +
#     labs(
#       title = paste("Relationship Between", cat_var, "and Numeric Variables"),
#       x = cat_var,
#       y = "Value"
#     ) +
#     theme_minimal()
#   
#   # Print the plot
#   print(plot)
# }

```

The relationship between snow and rainfall and whether it's a holiday or not is weird! Other than that, there isn't anything way out of the ordinary.

```{r}
#Looking at how total rent count relates to the other variables 
  
#   #with numeric variables using GGally package 
# ggpairs(bike_numeric, title = "Scatterplot Matrix: Total Rental Count and Numeric Variables",
#         #first time I printed everything was way too big for screen 
#         lower = list(continuous = wrap("points", size = 0.5, alpha = 0.3)), #adjust point size for each scatter plot 
#         upper = list(continuous = wrap("cor", size = 3)) #adjust size of the corr. statistics in each box 
# ) + theme(
#    axis.text = element_text(size = 6), #smaller axis labels 
#    strip.text = element_text(size = 6) #smaller facet labels 
#  )
# 
#   #with categorical variables 
# for (cat_var in categorical_vars) {
#   #boxplot for each categorical variable
#   plot <- ggplot(bikedata_summary, aes_string(x = cat_var, y = "total_rental_count")) +
#     geom_boxplot() +
#     labs(
#       title = paste("Total Rental Count by", cat_var),
#       x = cat_var,
#       y = "Total Rental Count"
#     ) +
#     theme_minimal()
#   
#   #print the plot
#   print(plot)
# }

```

These both makes logistical sense.

### 3. Splitting the data

```{r}
#split the data into training (75%) and testing (25%) sets, stratified by 'seasons'
set.seed(123)  # Set a seed for reproducibility
bike_split <- initial_split(bikedata_summary, prop = 0.75, strata = seasons)

#extract the training and testing sets
train_data <- training(bike_split)
test_data <- testing(bike_split)

#on the training data, create a 10-fold CV split 
cv_split <- vfold_cv(train_data, v = 10, strata = seasons)

#checking the structure of the cross-validation splits
cv_split
```

### 4. Fitting three different models

-   Here, we will also fit the models using 10-fold cross-validation to determine the best model.

```{r error=FALSE, warning= FALSE}
#Recipe #1 ---------------

  #fixing the date column
bike_1_recipe <- recipe(total_rental_count ~ ., data = bikedata_summary) %>%
  #extract the day of the week from the date variable
  step_date(date, features = "dow", label = TRUE) %>%
  #create a new factor variable 'weekday_weekend'
  step_mutate(
    weekday_weekend = factor(if_else(date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) %>%
  #remove the intermediate 'dow' variable and the original 'date' variable
  step_rm(date_dow, date) %>% 
  #standardize numeric vars 
  step_normalize(all_numeric()) %>% 
  #dummy variables 
  step_dummy(all_nominal_predictors())

#prepare and bake the recipe
first_recipe<- prep(bike_1_recipe)
# bike_1_recipe
bake(first_recipe, bikedata_summary)

#Recipe #2 ---------------

bike_2_recipe <- recipe(total_rental_count ~ ., data = bikedata_summary) %>%
  step_date(date, features = "dow", label = TRUE) %>%
  step_mutate(
    weekday_weekend = factor(if_else(date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) %>%
  step_rm(date_dow, date) %>% 
  step_normalize(all_numeric()) %>% 
  
  ######add interaction terms 
  step_interact(~starts_with("seasons"):holiday) %>% 
  step_interact(~starts_with("seasons"):avg_temperature) %>% 
  step_interact(~avg_temperature:total_rainfall) %>% 

  #dummy variables 
  step_dummy(all_nominal_predictors())

# prep(bike_2_recipe)

#Recipe #3 ---------------

bike_3_recipe <- recipe(total_rental_count ~ ., data = bikedata_summary) %>%
  step_date(date, features = "dow", label = TRUE) %>%
  step_mutate(
    weekday_weekend = factor(if_else(date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) %>%
  step_rm(date_dow, date) %>% 
  step_normalize(all_numeric()) %>% 

  step_interact(~starts_with("seasons"):holiday) %>% 
  step_interact(~starts_with("seasons"):avg_temperature) %>% 
  step_interact(~avg_temperature:total_rainfall) %>% 
  
  ######add quadratic terms for each numeric predictor 
  step_poly(all_numeric_predictors(), degree = 2, options = list(raw = TRUE)) %>% 
  
  #dummy variables 
  step_dummy(all_nominal_predictors())

# prep(bike_3_recipe)


#Set up linear model fit to use the 'lm' engine 
recipe_model<- linear_reg() %>% 
  set_engine("lm")

#create recipe workflows   
recipe_1_wfl <- workflow() %>% 
  add_recipe(bike_1_recipe) %>% 
  add_model(recipe_model)
# recipe_1_wfl
  
recipe_2_wfl <- workflow() %>% 
  add_recipe(bike_2_recipe) %>% 
  add_model(recipe_model)
# recipe_2_wfl
  
recipe_3_wfl <- workflow() %>% 
  add_recipe(bike_3_recipe) %>% 
  add_model(recipe_model)
# recipe_3_wfl
  

#Fit the models using 10 fold CV via fit_resamples() 
rec_10_fold <- vfold_cv(train_data, 10)
  
rec1_fits <- recipe_1_wfl %>% 
  fit_resamples(rec_10_fold)
  
rec2_fits <- recipe_2_wfl %>% 
  fit_resamples(rec_10_fold) 
  
rec3_fits <- recipe_3_wfl %>% 
  fit_resamples(rec_10_fold)
 
#collect metrics of the three models 
# rbind(
  # rec1_fits %>% collect_metrics(),
  # rec2_fits %>% collect_metrics(),
  # rec3_fits %>% collect_metrics())
```

Looking at the metrics of the three models, the best model is model 2 with the lowest rmse and highest value of R-squared.

### 5. Fitting the best MLR model

-   Here, we will fit the best model to the entire training data set

    -   we will additionally compute the RMSE metric on the test set and obtain the model (fit on the entire training set) coefficient table

```{r warning = FALSE}

#fitting on the training set bbnfgb
final_fit <- recipe_2_wfl %>% last_fit(split = bike_split, metrics = metric_set(rmse,mae))

#finding test set metrics
final_fit %>% collect_metrics()

#storing metrics as a table 
MLR_coef <- final_fit %>% collect_metrics()

#obtaining the final model fit 
final_MLR_model <- final_fit %>% extract_fit_parsnip()

#tidy table of coefficients 
tidy(final_MLR_model)

```

### Conclusions:

The RMSE metric of the test set is 0.301. The R-squared value is 0.91, meaning the model explains 91% of the variance in the total_rental_count. The coefficient table shows each coefficient/estimate for the predictors in the model.

## Homework 9 Extension

### 6. Tuned LASSO Model

```{r}
#set up how we'll fit LASSO model 
lasso_recipe <- recipe(total_rental_count ~., data = bikedata_summary) %>% 
    step_date(date, features = "dow", label = TRUE) %>%
  step_mutate(
    weekday_weekend = factor(if_else(date_dow %in% c("Sat", "Sun"), "Weekend", "Weekday"))
  ) %>%
  step_rm(date_dow, date) %>% 
  step_normalize(all_numeric()) %>%
  step_dummy(all_nominal_predictors())

#create a model instance 
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

#create workflow
lasso_wkf <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)
lasso_wkf

#fitting the model using tune_grid (fitting model to CV folds)
lasso_grid <- lasso_wkf %>%
  tune_grid(resamples= cv_split,
            grid=grid_regular(penalty(), levels = 200))
# lasso_grid

#collect metrics 
lasso_grid %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse")

#pulling out the best lasso model
lowest_rmse <- lasso_grid %>%
  select_best(metric = "rmse")
lowest_rmse

#fit the best model on the entire training set 
lasso_final <- lasso_wkf %>%
  finalize_workflow(lowest_rmse) %>%
  fit(train_data)
# tidy(lasso_final)

```

Final LASSO model coefficient table:

```{r}
lasso_wkf %>% finalize_workflow(lowest_rmse) %>% last_fit(bike_split, metrics = metric_set(rmse,mae)) %>% collect_metrics()

#storing as a table- 
lasso_coef <- lasso_wkf %>% finalize_workflow(lowest_rmse) %>% last_fit(bike_split, metrics = metric_set(rmse,mae)) %>% collect_metrics()

```

### 7. Tuned Regression Tree Model

```{r}
#same recipe as used in lasso 

#define model and engine
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) %>% 
  set_engine("rpart") %>%
  set_mode("regression")
  
#create workflow
regtree_wkf <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(tree_mod)

#use CV to select tuning parameters
temp <- regtree_wkf %>% tune_grid(resamples = cv_split)
temp %>% collect_metrics()

regtree_grid <- grid_regular(cost_complexity(),
                             tree_depth(),
                             levels = c(10, 5))

regtree_fits <- regtree_wkf %>%
  tune_grid(resamples = cv_split,
            grid = regtree_grid)

# regtree_fits %>% collect_metrics() %>% filter(.metric == "rmse") %>% arrange(mean)

#grab the best model's tuning parameters
regtree_best_param <- select_best(regtree_fits)
regtree_best_param

#finalize best model on the training set 
  #fit the best model
regtree_final_wkf <- regtree_wkf %>% 
  finalize_workflow(regtree_best_param)
  #fit on entire training set 
regtree_final_fit <- regtree_final_wkf %>% 
  last_fit(bike_split, metrics = metric_set(rmse, mae))
regtree_final_fit %>% collect_metrics()

#storing as a table- 
regtree_coef <- regtree_final_fit %>% collect_metrics()
```

Plot of the final regression tree model:

```{r}
regtree_final_fit %>% 
  extract_fit_engine() %>%
  rpart.plot::rpart.plot(roundint=FALSE)
```

### 8. Tuned Bagged Tree Model

```{r}
#same recipe used as previous 

#define model type/engine
bag_spec <- bag_tree(tree_depth = 5, min_n= 10, cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

#create workflow
bag_wkf <- workflow() %>% 
  add_recipe(lasso_recipe) %>%
  add_model(bag_spec)

#fit to cv folds 
bag_fit <- bag_wkf %>% 
  tune_grid(resamples = cv_split, 
            grid = grid_regular(cost_complexity(),
                                levels = 15),
            metrics = metric_set(rmse, mae))

# bag_fit %>% 
  # collect_metrics() %>% 
  # filter(.metric == "rmse") %>% 
  # arrange(mean)

#retrieve best tuning parameter
bag_best_param <- select_best(bag_fit)

#refit on entire training set 
bag_final_wkf <- bag_wkf %>% 
  finalize_workflow(bag_best_param)

bag_final_fit <- bag_final_wkf %>% 
  last_fit(bike_split, metrics = metric_set(rmse,mae))

collect_metrics(bag_final_fit)
#storing as table 
bag_coef<- collect_metrics(bag_final_fit)
```

Variable Importance Plot:

```{r}
#extract the final model and plot 
bag_final_model <- extract_fit_engine(bag_final_fit)
bag_final_model$imp %>%
  mutate(term=factor(term,levels=term)) %>% 
  ggplot(aes(x=term,y=value)) +
  geom_bar(stat = "identity") +
  coord_flip()
#avg temp has biggest play in the outcome! 
```

### 9. Tuned Random Forest Model

```{r}
#same recipe as previous 
#model specs - i found i had to add in the extra importance object to be able to get the variable importance info later on 
rf_spec <- rand_forest(mtry=tune()) %>% 
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("regression")

#create workflow 
rf_wkf <- workflow() %>% 
  add_recipe(lasso_recipe) %>% 
  add_model(rf_spec)

#fit to cv folds
rf_fit <- rf_wkf %>% 
  tune_grid(resamples = cv_split, 
            grid = 7,
            metrics = metric_set(rmse,mae))

#looking at metrics across folds 
rf_fit %>% collect_metrics() %>% arrange(mean)

#get best tuning parameter
rf_best_param <- select_best(rf_fit)

#refit on entire training set with this param
rf_final_wkf <- rf_wkf %>% 
  finalize_workflow(rf_best_param)

rf_final_fit <- rf_final_wkf %>% 
  last_fit(bike_split, metrics = metric_set(rmse,mae))
collect_metrics(rf_final_fit)
#saving as table 
rf_coef<- collect_metrics(rf_final_fit)
```

Variable Importance Plot:

```{r}
#extract the final model and plot 
rf_final_model <- extract_fit_engine(rf_final_fit)
#earlier used code didnt work for this model
impor_data<- as.data.frame(rf_final_model$variable.importance) %>% rownames_to_column("Variable") %>%
  rename(value = "rf_final_model$variable.importance") %>% 
  arrange(desc(value)) 

ggplot(impor_data, aes(x = reorder(Variable, value), y = value)) +
  geom_bar(stat = "identity") +
  coord_flip() 

#This model also suggests temp has the biggest roll, by a lot 
```

### 10. Comparing all final models (including the best MLR model from homework 8)

```{r}
#add model names as a new column
MLR_coef <- MLR_coef %>% mutate(model = "MLR")
lasso_coef <- lasso_coef %>% mutate(model = "Lasso")
regtree_coef <- regtree_coef %>% mutate(model = "Regression Tree")
bag_coef <- bag_coef %>% mutate(model = "Bagging")
rf_coef <- rf_coef %>% mutate(model = "Random Forest")
rbind(MLR_coef,lasso_coef,regtree_coef,bag_coef,rf_coef)
```

**The random forest model did the best, with a RMSE of 0.250 and a MAE of 0.206. The bagging tree method was very close to this method on these metrics. The Lasso model performed the worst.**

## 11. Fitting the overall best model to the entire data set

```{r}
best_fit_ever <- bag_final_wkf %>% 
  fit(bikedata_summary)
best_fit_ever

best_final_model <- extract_fit_engine(best_fit_ever)
best_final_model$imp %>%
  mutate(term=factor(term,levels=term)) %>% 
  ggplot(aes(x=term,y=value)) +
  geom_bar(stat = "identity") +
  coord_flip()

#woohoo
```
